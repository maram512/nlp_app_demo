# -*- coding: utf-8 -*-
"""word_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Tu6YfcKK5mTPT89XYJs9xaIWihsGOJEO
"""

import pandas as pd
import re

# Load persona dataset
persona_df = pd.read_csv("/content/personality.csv")

# Load empathetic dialogues dataset
empathetic_df = pd.read_csv("/content/emotion-emotion_69k.csv")

print("Persona rows:", len(persona_df))
print("Empathetic rows:", len(empathetic_df))

# Extract chat text from persona dataset
persona_chats = persona_df["chat"].astype(str).tolist()

# Extract dialogues from empathetic dataset
empathetic_chats = empathetic_df["empathetic_dialogues"].astype(str).tolist()


print("Persona sample:\n", persona_chats[0][:300])
print("\nEmpathetic sample:\n", empathetic_chats[0][:300])

def clean_text(text):
    """
    Clean chat text:
    - Lowercase
    - Remove excessive spaces
    - Remove non-English characters
    - Remove 'customer:' and 'agent:' tags
    """
    text = text.lower()
    text = re.sub(r"customer\s*:", "", text)
    text = re.sub(r"agent\s*:", "", text)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^\x00-\x7F]+", "", text)  # remove non-ASCII
    return text.strip()

# Clean persona chats
persona_cleaned = [clean_text(t) for t in persona_chats]

# Clean empathetic chats
empathetic_cleaned = [clean_text(t) for t in empathetic_chats]

print("Cleaned persona sample:\n", persona_cleaned[0][:300])
print("\nCleaned empathetic sample:\n", empathetic_cleaned[0][:300])

# Merge all cleaned chats into one list
all_dialogues = persona_cleaned + empathetic_cleaned

print("Total merged dialogues:", len(all_dialogues))

output_path = "/content/merged_dataset.txt"

with open(output_path, "w", encoding="utf-8") as f:
    for dialog in all_dialogues:
        f.write(dialog + "\n\n")  # Double newline = separation between conversations

print("Merged dataset saved as:", output_path)

#from google.colab import files
#files.download("/content/merged_dataset.txt")

# Path to merged dataset
file_path = "/content/merged_dataset.txt"

# Number of lines to display
num_lines = 5

# Open the file and print the first num_lines lines
with open(file_path, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= num_lines:
            break
        print(line.strip())

!pip install transformers datasets accelerate wandb --upgrade

import torch
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import matplotlib.pyplot as plt

# Path to merged dataset
file_path = "/content/merged_dataset.txt"

# Read all dialogues
with open(file_path, "r", encoding="utf-8") as f:
    lines = [line.strip() for line in f if line.strip()]

print(f"Total lines: {len(lines)}")
print("Sample line:\n", lines[0])

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add padding token if not exists
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Load GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2")

from sklearn.model_selection import train_test_split

class ChatDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=128):
        self.input_ids = []
        self.attn_masks = []

        for txt in texts:
            encodings_dict = tokenizer(
                txt,
                truncation=True,
                max_length=max_length,
                padding="max_length"
            )
            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))
            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attn_masks[idx],
            'labels': self.input_ids[idx]  # for causal LM
        }

# Split data into training and validation sets
train_texts, val_texts = train_test_split(lines, test_size=0.1, random_state=42)

# Create datasets
train_dataset = ChatDataset(train_texts, tokenizer)
val_dataset = ChatDataset(val_texts, tokenizer)

from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling

training_args = TrainingArguments(
    output_dir="./gpt2-finetuned",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=2,      # Small batch
    gradient_accumulation_steps=2,      # Simulate batch size 4
    save_steps=500,
    save_total_limit=2,
    logging_steps=100,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=100,
    prediction_loss_only=True,
    fp16=True,                          # Mixed precision for faster training
    report_to=[]                         # Disable wandb / logging
)

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=data_collator,
)

num_epochs = 2
save_every = 1
train_loss_history = []
eval_loss_history = []

for epoch in range(num_epochs):
    print(f"\n===== Epoch {epoch+1}/{num_epochs} =====")

    # Train one epoch
    trainer.train()

    # Get last training loss
    last_loss = None
    for item in reversed(trainer.state.log_history):
        if "loss" in item:
            last_loss = item["loss"]
            break

    # Evaluate
    eval_results = trainer.evaluate()
    eval_loss = eval_results.get("eval_loss", None)

    # Store losses
    train_loss_history.append(last_loss)
    eval_loss_history.append(eval_loss)

    # Print nicely
    train_str = f"{last_loss:.4f}" if last_loss is not None else "N/A"
    eval_str = f"{eval_loss:.4f}" if eval_loss is not None else "N/A"
    print(f"Epoch {epoch+1} → Train Loss = {train_str}, Eval Loss = {eval_str}")

    # Save checkpoint every `save_every` epochs
    if (epoch + 1) % save_every == 0:
        checkpoint_dir = f"./checkpoint_epoch{epoch+1}"
        trainer.save_model(checkpoint_dir)
        tokenizer.save_pretrained(checkpoint_dir)
        print(f"Checkpoint saved at {checkpoint_dir}")

# ---------------- Plot Losses ----------------
plt.plot(range(1, num_epochs+1), train_loss_history, label="Train Loss")
plt.plot(range(1, num_epochs+1), eval_loss_history, label="Eval Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training & Evaluation Loss per Epoch")
plt.legend()
plt.show()

# ---------------- Save Final Model ----------------
trainer.save_model("./gpt2-finetuned-final")
tokenizer.save_pretrained("./gpt2-finetuned-final")

!zip -r gpt2-finetuned-final.zip gpt2-finetuned-final
from google.colab import files
files.download("gpt2-finetuned-final.zip")

"""# Full Text Generation"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import re

device = "cuda"

# GPT-2 requires this
tokenizer.pad_token = tokenizer.eos_token

while True:
    user_prompt = input("Enter prompt (or 'exit'): ")
    if user_prompt.lower() == "exit":
        break

    prompt = "Continue the sentence: " + user_prompt

    # Encode the prompt to tensors and move to device
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    # Generate text using the model
    outputs = model.generate(
      inputs.input_ids,
      max_new_tokens=30,
      do_sample=True,
      temperature=0.7,         # Lower → more coherent and logical text
      top_p=0.95,              # Covers a large portion of the distribution
      no_repeat_ngram_size=3,  # Prevent repeating n-grams
      repetition_penalty=1.2,  # Discourage word repetition
      pad_token_id=tokenizer.eos_token_id
    )

    # Decode the output tokens to string
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Keep only the first sentence after the prompt
    generated_part = full_text.replace(prompt, "").strip()
    first_sentence = re.split(r"[.!?]", generated_part)[0]

    print("\nGenerated:")
    print(first_sentence.strip())
    print("-" * 50)

