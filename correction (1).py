# -*- coding: utf-8 -*-
"""correction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AhK5e6BEBPMy3j_TScg2dHxKRGqgHWpY
"""

import json

# Paths to your files
corrupt_path = "//content/sample_corrupt.txt"
clean_path = "/content/sample_clean.txt"

# Read files
with open(corrupt_path, "r", encoding="utf-8") as f:
    corrupt_lines = f.readlines()

with open(clean_path, "r", encoding="utf-8") as f:
    clean_lines = f.readlines()

# Check that both files have the same number of lines
if len(corrupt_lines) != len(clean_lines):
    raise ValueError("The number of lines in corrupt and clean files do not match!")

# Create JSON data
data_json = []
for corrupt, clean in zip(corrupt_lines, clean_lines):
    data_json.append({
        "input": corrupt.strip(),
        "output": clean.strip()
    })

# Save to JSON file
json_path = "/content/sample_dataset.json"
with open(json_path, "w", encoding="utf-8") as f:
    json.dump(data_json, f, ensure_ascii=False, indent=4)

print(f"Saved {len(data_json)} examples to {json_path}")

import json
import random

# Correct sentences pool (larger and more diverse)
correct_sentences = [
    "I am happy to see you today.",
    "She bought a new book from the store.",
    "We are planning a picnic next weekend.",
    "The cat is sleeping on the sofa.",
    "He enjoys playing football every Saturday.",
    "They visited the museum yesterday afternoon.",
    "I can't believe how beautiful the sunset is.",
    "Please remember to bring your notebook tomorrow.",
    "My favorite food is spaghetti with tomato sauce.",
    "The students are preparing for their final exams.",
    "She is learning to play the piano.",
    "He made a delicious cake for his friend's birthday.",
    "The children are playing in the park near the river.",
    "I have never seen such an amazing performance before.",
    "We should leave early to avoid the traffic.",
    "The weather forecast predicts rain this evening.",
    "She found a wallet on the street and returned it to the owner.",
    "He is writing a letter to his grandmother.",
    "I like walking my dog in the mornings.",
    "They are organizing a surprise party for their colleague."
]

# Typo transformations
typo_map = {
    "I": ["i", "I"],
    "am": ["am", "am", "aam"],
    "happy": ["happy", "hapy", "happpy"],
    "to": ["to", "too", "t0"],
    "see": ["see", "sea", "see"],
    "you": ["you", "yu", "yuo"],
    "today": ["today", "todaay", "todai"],
    "She": ["She", "she", "Shhe"],
    "bought": ["bought", "bougth", "baut"],
    "a": ["a", "a", "aa"],
    "new": ["new", "neew", "nwe"],
    "book": ["book", "bokk", "boook"],
    "from": ["from", "frm", "fro"],
    "the": ["the", "teh", "th"],
    "store": ["store", "stor", "storre"],
    "We": ["We", "we", "Wwe"],
    "are": ["are", "ar", "aer"],
    "planning": ["planning", "planing", "plannig"],
    "picnic": ["picnic", "picnik", "piknic"],
    "next": ["next", "nex", "neext"],
    "weekend": ["weekend", "wekend", "weeknd"],
    "The": ["The", "Teh", "Th"],
    "cat": ["cat", "kat", "catt"],
    "is": ["is", "iz", "si"],
    "sleeping": ["sleeping", "slepning", "slping"],
    "on": ["on", "oen", "on"],
    "sofa": ["sofa", "soffa", "sofaa"],
    "He": ["He", "he", "Hhe"],
    "enjoys": ["enjoys", "enjys", "enjois"],
    "playing": ["playing", "plaing", "playng"],
    "football": ["football", "fotball", "footbal"],
    "every": ["every", "evry", "evrey"],
    "Saturday": ["Saturday", "Saturdy", "Satrday"],
    "They": ["They", "thay", "Tey"],
    "visited": ["visited", "vistied", "visitted"],
    "museum": ["museum", "musuem", "musem"],
    "yesterday": ["yesterday", "yestarday", "yestaday"],
    "afternoon": ["afternoon", "afternon", "afternoone"],
    "can't": ["can't", "cant", "cna't"],
    "believe": ["believe", "beleive", "beliive"],
    "how": ["how", "hou", "hw"],
    "beautiful": ["beautiful", "beutiful", "beautifull"],
    "sunset": ["sunset", "sunst", "sunsett"],
    "Please": ["Please", "Plase", "Plese"],
    "remember": ["remember", "remembr", "remeber"],
    "bring": ["bring", "brng", "brinng"],
    "your": ["your", "yoru", "yuor"],
    "notebook": ["notebook", "notebok", "notebokk"],
    "tomorrow": ["tomorrow", "tomorow", "tomoroww"],


# ✅ Generate dataset
dataset = []
for _ in range(500):  # generate 500 examples
    correct = random.choice(correct_sentences)
    words = correct.split()
    corrupt_words = [random.choice(typo_map.get(word, [word])) for word in words]
    corrupt_sentence = " ".join(corrupt_words)
    dataset.append({
        "input": corrupt_sentence,
        "output": correct
    })

# ✅ Save JSON
with open("/content/generated_typo_dataset_diverse.json", "w", encoding="utf-8") as f:
    json.dump(dataset, f, ensure_ascii=False, indent=4)

print("Generated 500 diverse sentences with typos saved to /content/generated_typo_dataset_diverse.json")

import json
import zstandard as zstd
import random
from sklearn.model_selection import train_test_split

# ==========================
# Function to read zst file (jsonl.zst)
# ==========================
def read_zst_jsonl(file_path):
    data = []
    with open(file_path, "rb") as f:
        compressed_content = f.read()

    dctx = zstd.ZstdDecompressor()
    decompressed_bytes = dctx.decompress(compressed_content)
    decompressed_text = decompressed_bytes.decode("utf-8")

    for line in decompressed_text.strip().split('\n'):
        if line:
            data.append(json.loads(line))
    return data

# ==========================
# Load JSON datasets
# ==========================
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# Load datasets
gen_diverse = load_json("/content/generated_typo_dataset_diverse.json")
sample_data = load_json("/content/sample_dataset.json")
train_data_extra = read_zst_jsonl("/content/train.jsonl.zst")  # additional training dataset
val_data_original = read_zst_jsonl("/content/validation.jsonl.zst")  # for final testing

# ==========================
# Merge generated, sample, and extra train data
# ==========================
merged_train_candidates = gen_diverse + sample_data + train_data_extra
random.shuffle(merged_train_candidates)

# ==========================
# Split merged data into train and temp (10% for test)
# ==========================
train_split_ratio = 0.9
train_data, temp_data = train_test_split(merged_train_candidates, train_size=train_split_ratio, random_state=42)

# ==========================
# Final testing dataset = temp_data + original validation
# ==========================
test_data = temp_data + val_data_original
random.shuffle(test_data)

print(f"Training examples: {len(train_data)}")
print(f"Testing examples: {len(test_data)}")

# ==========================
# Save final datasets
# ==========================
train_output_path = "/content/final_train_dataset.json"
test_output_path = "/content/final_test_dataset.json"

with open(train_output_path, "w", encoding="utf-8") as f:
    json.dump(train_data, f, ensure_ascii=False, indent=4)

with open(test_output_path, "w", encoding="utf-8") as f:
    json.dump(test_data, f, ensure_ascii=False, indent=4)

print(f"Training dataset saved to {train_output_path}")
print(f"Testing dataset saved to {test_output_path}")

# ==========================
# Display some examples
# ==========================
print("\nSome examples from training dataset:")
for i, ex in enumerate(train_data[:5]):
    print(f"Train Example {i+1}:")
    print(f"Input: {ex['input']}")
    print(f"Output: {ex['output']}")
    print("-" * 50)

print("\nSome examples from testing dataset:")
for i, ex in enumerate(test_data[:5]):
    print(f"Test Example {i+1}:")
    print(f"Input: {ex['input']}")
    print(f"Output: {ex['output']}")
    print("-" * 50)

# ==========================
# Download datasets from Colab
# ==========================
from google.colab import files
files.download(train_output_path)
files.download(test_output_path)

!pip install transformers datasets torch tqdm matplotlib

import json
from datasets import Dataset
from transformers import T5Tokenizer

# Load your final training and testing datasets
with open("/content/final_train_dataset.json", "r", encoding="utf-8") as f:
    train_data = json.load(f)

with open("/content/final_test_dataset.json", "r", encoding="utf-8") as f:
    test_data = json.load(f)

# Convert to HuggingFace Dataset
train_dataset = Dataset.from_list(train_data)
test_dataset = Dataset.from_list(test_data)

# Initialize T5 tokenizer
model_name = "t5-small"
tokenizer = T5Tokenizer.from_pretrained(model_name)

# Maximum input and output length
max_input_len = 128
max_output_len = 128

# Preprocessing function
def preprocess_function(examples):
    inputs = examples['input']
    targets = examples['output']
    model_inputs = tokenizer(inputs, max_length=max_input_len, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=max_output_len, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply preprocessing
train_dataset = train_dataset.map(preprocess_function, batched=True)
test_dataset = test_dataset.map(preprocess_function, batched=True)

# Set the format to PyTorch tensors
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

from torch.utils.data import DataLoader

batch_size = 8

train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)
eval_dataloader = DataLoader(test_dataset, batch_size=batch_size)

import torch
from transformers import T5ForConditionalGeneration
from torch.optim import AdamW

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = T5ForConditionalGeneration.from_pretrained(model_name)
model = model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

import os
from tqdm import tqdm

epochs = 3
save_dir = "/content/t5_checkpoints"
os.makedirs(save_dir, exist_ok=True)

train_losses = []
val_losses = []

accumulation_steps = 4  # accumulate gradients over 4 mini-batches

for epoch in range(1, epochs+1):
    model.train()
    total_train_loss = 0
    optimizer.zero_grad()  # reset gradients before epoch

    for step, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch} Training")):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss = loss / accumulation_steps  # scale loss
        loss.backward()
        total_train_loss += loss.item() * accumulation_steps  # scale back for logging

        if (step + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()

    # In case last few steps < accumulation_steps
    if (step + 1) % accumulation_steps != 0:
        optimizer.step()
        optimizer.zero_grad()

    avg_train_loss = total_train_loss / len(train_dataloader)
    train_losses.append(avg_train_loss)

    # Validation
    model.eval()
    total_val_loss = 0
    with torch.no_grad():
        for batch in tqdm(eval_dataloader, desc=f"Epoch {epoch} Validation"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(eval_dataloader)
    val_losses.append(avg_val_loss)

    print(f"\nEpoch {epoch} | Train Loss: {avg_train_loss:.4f} | Validation Loss: {avg_val_loss:.4f}")

    # Save checkpoint
    checkpoint_path = os.path.join(save_dir, f"epoch_{epoch}")
    model.save_pretrained(checkpoint_path)
    tokenizer.save_pretrained(checkpoint_path)
    print(f"Checkpoint saved at {checkpoint_path}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.plot(range(1, epochs+1), train_losses, label="Train Loss")
plt.plot(range(1, epochs+1), val_losses, label="Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training & Validation Loss")
plt.legend()
plt.show()

from zipfile import ZipFile

# Save final model
final_model_path = "/content/final_t5_model"
model.save_pretrained(final_model_path)
tokenizer.save_pretrained(final_model_path)

# Zip the folder
import shutil
shutil.make_archive(final_model_path, 'zip', final_model_path)

# Download
from google.colab import files
files.download(final_model_path + ".zip")

from transformers import T5ForConditionalGeneration, T5Tokenizer

# Load fine-tuned model and tokenizer
model_path = "/content/final_t5_model"
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path)

while True:
    text = input("Enter text (or 'exit'): ")
    if text.lower() == "exit":
        break

    # Prepare input for T5
    encoded = tokenizer(
        text,
        return_tensors="pt",
        padding=True,
        truncation=True
    )

    # Generate corrected sentence
    output = model.generate(
        input_ids=encoded.input_ids,
        attention_mask=encoded.attention_mask,
        max_length=128,
        num_beams=5
    )

    # Decode result
    corrected = tokenizer.decode(output[0], skip_special_tokens=True)

    print("Corrected:", corrected)

